{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac64f10c",
   "metadata": {},
   "source": [
    "# Twitter NLP Sentiment Classifier\n",
    "\n",
    "This notebook builds a simple NLP model to classify tweets as positive, neutral, or negative using the [Twitter140 dataset](https://www.kaggle.com/datasets/kazanova/sentiment140)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0155e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import basic core libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d080874a",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Data Preprocessing ðŸ”¨\n",
    "\n",
    "#### Steps ðŸ“„:\n",
    "- Load and inspect Sentiment140 dataset\n",
    "\n",
    "- Clean raw tweet text\n",
    "\n",
    "- Normalize case and remove stopwords\n",
    "\n",
    "- Split into training, validation, and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340513be",
   "metadata": {},
   "source": [
    "We'll begin by loading our dataframe and exploring the dataset to get an understanding of its structure and the distribution of sentiment labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "164a744d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0           1                             2         3                4  \\\n",
      "0  0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
      "1  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
      "2  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
      "3  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
      "4  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
      "\n",
      "                                                   5  \n",
      "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1  is upset that he can't update his Facebook by ...  \n",
      "2  @Kenichan I dived many times for the ball. Man...  \n",
      "3    my whole body feels itchy and like its on fire   \n",
      "4  @nationwideclass no, it's not behaving at all....  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0\n",
       "0    800000\n",
       "4    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/Sentiment140.csv\", encoding=\"latin-1\", header=None)\n",
    "\n",
    "print(df.head())\n",
    "df[0].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657516a6",
   "metadata": {},
   "source": [
    "We are only going to be using the sentiment and text, which are found in column 1 and 5 respectively.<br/>\n",
    "The sentiment is distributed evenly between 0 (negative) and 4 (positive).<br/>\n",
    "It is also clear that the text will need some cleaning (tags, urls, special characters, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce3302c",
   "metadata": {},
   "source": [
    "Lets label our columns and grab the ones we want. <br/>\n",
    "We'll also map all 4s to 1 for binary simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64caf399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment                                               text\n",
      "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
      "1          0  is upset that he can't update his Facebook by ...\n",
      "2          0  @Kenichan I dived many times for the ball. Man...\n",
      "3          0    my whole body feels itchy and like its on fire \n",
      "4          0  @nationwideclass no, it's not behaving at all....\n",
      "sentiment\n",
      "0    800000\n",
      "1    800000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df.columns = [\"sentiment\", \"id\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "df = df[[\"sentiment\", \"text\"]]\n",
    "\n",
    "df[\"sentiment\"] = df[\"sentiment\"].replace({4:1})\n",
    "\n",
    "print(df.head())\n",
    "print(df[\"sentiment\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf91ea8",
   "metadata": {},
   "source": [
    "Next we'll use the `regular expressions` module and `NLTK` library to clean the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90fcaf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sawyeralston/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        thats bummer shoulda got david carr third day\n",
      "1    upset cant update facebook texting might cry r...\n",
      "2    dived many times ball managed save rest go bounds\n",
      "3                     whole body feels itchy like fire\n",
      "4                             behaving im mad cant see\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# We are removing urls, tags (@), hashtags, and special characters from the tweets\n",
    "# We also remove stopwords, words that contrubute very little to the sentiment of the tweets (ex. \"the\", \"and\", or \"is\")\n",
    "def clean_tweets(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    text = text.lower().strip().split()\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    return \" \".join(text)\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(clean_tweets)\n",
    "print (df[\"text\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25ad4475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # Keep apostrophes and basic punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z'\\s]\", \"\", text)  # Allow apostrophes\n",
    "    words = text.lower().strip().split()\n",
    "    # Custom stop words - preserve sentiment carriers\n",
    "    custom_stopwords = set(stopwords.words('english')) - {'not', 'no', 'never', 'very'}\n",
    "    return [w for w in words if w not in custom_stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73994a60",
   "metadata": {},
   "source": [
    "Now we can split the data into train, validation, and test using stratified sampling to ensure fair evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7f62d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train split: sentiment\n",
      "1    0.501026\n",
      "0    0.498974\n",
      "Name: proportion, dtype: float64\n",
      "validation split: sentiment\n",
      "1    0.501042\n",
      "0    0.498958\n",
      "Name: proportion, dtype: float64\n",
      "test split: sentiment\n",
      "1    0.501\n",
      "0    0.499\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = df.sample(frac=0.1, random_state=1)  # use only 10% of the full dataset (temporary)\n",
    "\n",
    "# Split into train&val (90%) and the test set (10%)\n",
    "df_trainandval, df_test = train_test_split(df, test_size=0.1, stratify=df[\"sentiment\"], random_state=1)\n",
    "\n",
    "# Now split train&val into train (90% of 90% --> 81%) and val (10% of 90% --> 9%)\n",
    "df_train, df_val = train_test_split(df_trainandval, test_size=0.1, stratify=df_trainandval[\"sentiment\"], random_state=1)\n",
    "\n",
    "print(f\"train split: {df_train['sentiment'].value_counts(normalize=True)}\")\n",
    "print(f\"validation split: {df_val['sentiment'].value_counts(normalize=True)}\")\n",
    "print(f\"test split: {df_test['sentiment'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54261864",
   "metadata": {},
   "source": [
    "We will vectorize text at the beginning of our ML approach and tokenize at the start of our DL approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bc3609",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Classical ML Approach ðŸ’»\n",
    "\n",
    "#### Steps ðŸ“„:\n",
    "- Extract features with TF-IDF\n",
    "\n",
    "- Train logistic regression model\n",
    "\n",
    "- Evaluate model on validation and test set\n",
    "\n",
    "- Analyze performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bce7dcc",
   "metadata": {},
   "source": [
    "We will start our ML approach by vectorizing our text data and fitting our model to the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ddc8e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=15000)\n",
    "\n",
    "# These will be the features for our ML model, we fit on our training features\n",
    "ml_X_train = vectorizer.fit_transform(df_train[\"text\"])\n",
    "ml_X_val = vectorizer.transform(df_val[\"text\"])\n",
    "ml_X_test = vectorizer.transform(df_test[\"text\"])\n",
    "\n",
    "# The labels for our ML model\n",
    "ml_y_train = df_train[\"sentiment\"]\n",
    "ml_y_val = df_val[\"sentiment\"]\n",
    "ml_y_test = df_test[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52135809",
   "metadata": {},
   "source": [
    "Now we can take a look at the first 10 words inside the internal feature matrix and their respective IDF scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce8ae01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 words: ['studying', 'dates', 'suck', 'got', 'titles', 'majority', 'artists', 'though', 'woop', 'one']\n",
      "IDF Scores: [10.06416558 10.06416558 10.82630563 10.69277424 11.16277787 10.98045631\n",
      " 10.5749912  11.16277787 10.46963069  9.72769334]\n"
     ]
    }
   ],
   "source": [
    "print (f\"First 10 words: {list(vectorizer.vocabulary_.keys())[:10]}\")\n",
    "print (f\"IDF Scores: {vectorizer.idf_[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5fbcb7",
   "metadata": {},
   "source": [
    "Next we'll initialize and train a logistic regression model on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "015fdfdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(ml_X_train, ml_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be541c7b",
   "metadata": {},
   "source": [
    "Now lets get a baseline to see how our model performs on its training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2c993e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8028395061728395\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "ml_train_preds = clf.predict(ml_X_train)\n",
    "\n",
    "print (f\"Training Accuracy: {accuracy_score(ml_y_train, ml_train_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a65eef8",
   "metadata": {},
   "source": [
    "Our training accuracy is good, 75-80% is a nice range to fall in for this model on this dataset. We're just going to be using this model as a baseline to compare against our DL model so we don't necessarily need to fine tune the hyperparameters (unless we see an issue). Before we evaluate on our test set, though, lets check our validation accuracy to make sure the model isn't overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "755047c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7693055555555556\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.75      0.76      7185\n",
      "           1       0.76      0.79      0.77      7215\n",
      "\n",
      "    accuracy                           0.77     14400\n",
      "   macro avg       0.77      0.77      0.77     14400\n",
      "weighted avg       0.77      0.77      0.77     14400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ml_val_preds = clf.predict(ml_X_val)\n",
    "\n",
    "print (f\"Validation Accuracy: {accuracy_score(ml_y_val, ml_val_preds)}\")\n",
    "print (classification_report(ml_y_val, ml_val_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3616c9e9",
   "metadata": {},
   "source": [
    "Nice, it looks like the model isn't overfitting and is doing well at generalizing to our validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f96a85",
   "metadata": {},
   "source": [
    "Now lets evaluate the model's performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3b09b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7698125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.76      0.77      7984\n",
      "           1       0.76      0.78      0.77      8016\n",
      "\n",
      "    accuracy                           0.77     16000\n",
      "   macro avg       0.77      0.77      0.77     16000\n",
      "weighted avg       0.77      0.77      0.77     16000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ml_test_preds = clf.predict(ml_X_test)\n",
    "\n",
    "print (f\"Test Accuracy: {accuracy_score(ml_y_test, ml_test_preds)}\")\n",
    "print (classification_report(ml_y_test, ml_test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1862b405",
   "metadata": {},
   "source": [
    "Great, 77% accuracy is a nice baseline to compare to moving fowards. Now we can move to our DL approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4682b6f",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Deep Learning Approach ðŸ§ \n",
    "\n",
    "#### Steps ðŸ“„:\n",
    "- Tokenize and vectorize features\n",
    "\n",
    "- Create dataset and dataloaders\n",
    "\n",
    "- Build and train LSTM model\n",
    "\n",
    "- Test on validation and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82635ce9",
   "metadata": {},
   "source": [
    "Lets start by building our vocabulary which includes tokenization and numericalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c98bb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 11966\n",
      "First 3 words: studying, dates, suck\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, min_freq=5):\n",
    "        self.min_freq = min_freq\n",
    "        self.word2idx = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "        self.idx2word = {0: \"<pad>\", 1: \"<unk>\"}\n",
    "        self.idx = 2\n",
    "    \n",
    "    # Build vocabulary, updating word2idx and idx2word with words that appear >= min_freq\n",
    "    def build_vocab(self, tweets):\n",
    "        counter = Counter()\n",
    "        for tweet in tweets:\n",
    "            counter.update(tweet.lower().split())\n",
    "\n",
    "        for word, count in counter.items():\n",
    "            if count>= self.min_freq and word not in self.word2idx:\n",
    "                self.word2idx[word] = self.idx\n",
    "                self.idx2word[self.idx] = word\n",
    "                self.idx+= 1\n",
    "    \n",
    "    # Convert a given tweet to indexes\n",
    "    def numericalize(self, tweet):\n",
    "        tokens = tweet.lower().split()\n",
    "        return [self.word2idx.get(token, self.word2idx[\"<unk>\"]) for token in tokens]\n",
    "\n",
    "# Build vocab\n",
    "vocab = Vocabulary()\n",
    "vocab.build_vocab(df_train[\"text\"])\n",
    "\n",
    "# Check\n",
    "print (f\"Vocab size: {len(vocab.word2idx)}\")\n",
    "print (f\"First 3 words: {vocab.idx2word[2]}, {vocab.idx2word[3]}, {vocab.idx2word[4]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d09b5a",
   "metadata": {},
   "source": [
    "Great, now lets create our `Dataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7acf7d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# PyTorch Dataset for our DataLoaders\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, tweets, sentiments, vocab, max_len=100):\n",
    "        self.tweets = tweets\n",
    "        self.sentiments = sentiments\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tweets)\n",
    "    \n",
    "    # Convert to numerical tokens and return feature and label as tensor pair\n",
    "    def __getitem__(self, index):\n",
    "        text = self.tweets[index]\n",
    "        label = self.sentiments[index]\n",
    "\n",
    "        # Convert text to numericalized tokens\n",
    "        numericalized = self.vocab.numericalize(text)\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(numericalized, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.float)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7956fcc",
   "metadata": {},
   "source": [
    "Next lets make the collate function for our DataLoaders to pad our texts and batch our features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e922aaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "\n",
    "    # pads tweets to same length and returns as a stacked tensor (batch size, longest tweet)\n",
    "    padded_texts = pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "\n",
    "    # stacks labels into tensor\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    return padded_texts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180e438f",
   "metadata": {},
   "source": [
    "Now lets make the `DataLoaders` for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d001601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "max_len = 100\n",
    "batch_size = 32\n",
    "\n",
    "# Creates datasets\n",
    "train_dataset = TweetDataset(df_train[\"text\"].tolist(), df_train[\"sentiment\"].tolist(), vocab, max_len)\n",
    "val_dataset = TweetDataset(df_val[\"text\"].tolist(), df_val[\"sentiment\"].tolist(), vocab, max_len)\n",
    "test_dataset = TweetDataset(df_test[\"text\"].tolist(), df_test[\"sentiment\"].tolist(), vocab, max_len)\n",
    "\n",
    "# Wraps datasets in DataLoaders and shuffles training dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size, shuffle=False, collate_fn=collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbcf7f5",
   "metadata": {},
   "source": [
    "Before we move to building the model, lets verify our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb423214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 129600\n",
      "Validation samples: 14400\n",
      "Test samples: 16000\n",
      "Raw sample text: studying dates suck got titles majority artists though woop\n",
      "Processed sample text: tensor([ 2,  3,  4,  5,  6,  7,  8,  9, 10])\n",
      "Sample label: 0.0\n",
      "Texts tensor shape: torch.Size([32, 15])\n",
      "Labels tensor shape: torch.Size([32])\n",
      "First text in batch: <unk> part asking\n",
      "First label in batch: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Check our dataset lengths\n",
    "print (f\"Training samples: {len(train_dataset)}\")\n",
    "print (f\"Validation samples: {len(val_dataset)}\")\n",
    "print (f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Check the first training sample\n",
    "text, sentiment = train_dataset[0]\n",
    "print (f\"Raw sample text: {df_train['text'].iloc[0]}\")\n",
    "print (f\"Processed sample text: {text}\")\n",
    "print (f\"Sample label: {sentiment}\")\n",
    "\n",
    "# Check batch shapes\n",
    "batch_texts, batch_sentiments = next(iter(train_loader))\n",
    "print (f\"Texts tensor shape: {batch_texts.shape}\")\n",
    "print (f\"Labels tensor shape: {batch_sentiments.shape}\")\n",
    "\n",
    "#Check batch content\n",
    "first_text = [vocab.idx2word[i] for i in (batch_texts[0].tolist()) if i != vocab.word2idx[\"<pad>\"]]\n",
    "print (f\"First text in batch: {' '.join(first_text)}\")\n",
    "print (f\"First label in batch: {batch_sentiments[0].item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29902a73",
   "metadata": {},
   "source": [
    "Great, our datasets look good and our DataLoaders appear to be working. <br/>\n",
    "Feature tensors are the right shape (labels as well) and padding looks to be working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e70f25",
   "metadata": {},
   "source": [
    "Now lets move foward with making our LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26adf3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "      \n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pad_index, dropout_rate):\n",
    "        super().__init__()\n",
    "        # Embedding layer: converts tweets (stored as vocab indexes) to dense embedded vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, pad_index)\n",
    "        nn.init.uniform_(self.embedding.weight, -0.05, 0.05)\n",
    "        # LSTM layer: processes embedded input, learning sequential patterns\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        # Dropout layer: drops out percentage of neurons for regularization\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        # Fully connected layer: takes final hidden state from LSTM and converts to output classes\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        \n",
    "        last_state = self.dropout(lstm_out[:, -1, :])\n",
    "        return self.fc(last_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "28e50078",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "# Initialize\n",
    "model = LSTMClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim = 100,\n",
    "    hidden_dim = 128,\n",
    "    output_dim = 1,\n",
    "    pad_index = 0,\n",
    "    dropout_rate = 0.5\n",
    "    \n",
    ").to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Enhanced optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',patience=3)\n",
    "\n",
    "def train_loop(model, dataloader):\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        total_loss, total_correct, total_samples = 0, 0, 0\n",
    "        \n",
    "        for texts, labels in dataloader:\n",
    "            texts, labels = texts.to(device), labels.to(device).float().unsqueeze(1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * labels.shape[0]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                total_correct += (preds == labels).sum().item()\n",
    "                total_samples += labels.shape[0]\n",
    "        \n",
    "        avg_loss = total_loss / total_samples  # Weighted by sample count\n",
    "        epoch_accuracy = total_correct / total_samples * 100  # True epoch accuracy\n",
    "        scheduler.step(avg_loss)\n",
    "        return avg_loss, epoch_accuracy\n",
    "\n",
    "            \n",
    "        '''avg_loss = total_loss / len(train_loader)\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}, Acc = {accuracy}, LR = {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        print (len(outputs))'''\n",
    "        \n",
    "#train_loop(model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65347b9b",
   "metadata": {},
   "source": [
    "With our layers and forward pass defined, lets move on to defining our hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d765edd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print (df_train[\"sentiment\"].unique())\n",
    "print ((train_dataset[0])[0].dtype)\n",
    "\n",
    "model = LSTMClassifier(\n",
    "    vocab_size= len(vocab),\n",
    "    embedding_dim= 64,\n",
    "    hidden_dim= 128,\n",
    "    output_dim= 1,\n",
    "    pad_index= vocab[\"<pad>\"],\n",
    "    dropout= 0.2\n",
    ").to(device)\n",
    "\n",
    "# We are using BCE for our loss function to classify our logits from the FC layer w/ softmax \n",
    "# Adam is a good, simple optimizer for NLP tasks, especially when using LSTMs like this model\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58ffcff",
   "metadata": {},
   "source": [
    "With our model defined, lets build the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "5916a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, dataloader):\n",
    "    model.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    \n",
    "    for batch_x, batch_y in dataloader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(batch_x).squeeze(-1)\n",
    "        loss = criterion(preds, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pred_classes = (preds > 0).long()  # Threshold at 0, then cast to int64\n",
    "        total_correct += (pred_classes == batch_y).sum().item()\n",
    "        #total_correct += (preds.argmax(1) == batch_y).sum().item()\n",
    "    \n",
    "    return total_loss / len(dataloader), total_correct / len(dataloader.dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70933c3f",
   "metadata": {},
   "source": [
    "Next, lets make our evaluation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7055ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            preds = model(batch_x).squeeze(-1)\n",
    "            pred_classes = (preds > 0).long()  # Threshold at 0, then cast to int64\n",
    "            total_correct += (pred_classes == batch_y).sum().item()\n",
    "            #total_correct += (preds.argmax(1) == batch_y).sum().item()\n",
    "    return total_correct / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a0d5929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, verbose=False):\n",
    "    \"\"\"Enhanced evaluation with diagnostics.\n",
    "    \n",
    "    Args:\n",
    "        model: Your LSTM classifier\n",
    "        dataloader: Evaluation DataLoader\n",
    "        verbose: Print prediction distributions if True\n",
    "        \n",
    "    Returns:\n",
    "        (avg_loss, accuracy)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # For diagnostics\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for texts, labels in dataloader:\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "            \n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).float()\n",
    "            \n",
    "            total_loss += loss.item() * texts.size(0)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += texts.size(0)\n",
    "            \n",
    "            # Store for diagnostics\n",
    "            if verbose:\n",
    "                all_probs.append(probs.cpu())\n",
    "                all_labels.append(labels.cpu())\n",
    "    \n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = total_correct / total_samples\n",
    "    \n",
    "    # Diagnostic outputs\n",
    "    if verbose and len(all_probs) > 0:\n",
    "        all_probs = torch.cat(all_probs)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        \n",
    "        print(\"\\nValidation Prediction Distribution:\")\n",
    "        print(f\"- Mean probability: {all_probs.mean():.4f}\")\n",
    "        print(f\"- % in uncertain range [0.4-0.6]: {((all_probs > 0.4) & (all_probs < 0.6)).float().mean():.2%}\")\n",
    "        print(f\"- Correct predictions mean confidence: {all_probs[all_labels == 1].mean():.4f}\")\n",
    "        print(f\"- Wrong predictions mean confidence: {all_probs[all_labels == 0].mean():.4f}\")\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "63d77391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0+cu118\n",
      "True\n",
      "11.8\n",
      "sentiment\n",
      "1    64933\n",
      "0    64667\n",
      "Name: count, dtype: int64\n",
      "original text: studying dates suck got titles majority artists though woop\n",
      "Tokenized text: ['studying', 'dates', 'suck', 'got', 'titles', 'majority', 'artists', 'though', 'woop']\n",
      "Vocab size: 75083\n",
      "Sample words: ['<pad>', '<unk>', 'im', 'good', 'day', 'get', 'like', 'go', 'dont', 'love', 'today', 'work', 'going', 'cant', 'got', 'back', 'time', 'lol', 'u', 'one']\n",
      "------------\n",
      "sentiment\n",
      "1    7215\n",
      "0    7185\n",
      "Name: count, dtype: int64\n",
      "original text: note exhaustive succesful effort get netbook back online im offlining spell bye folks\n",
      "Tokenized text: ['note', 'exhaustive', 'succesful', 'effort', 'get', 'netbook', 'back', 'online', 'im', 'offlining', 'spell', 'bye', 'folks']\n",
      "Epoch 1: Training Acc = 73.65586419753086 | Val Acc = (0.48840990291701425, 0.763125) | Training Loss = 0.52727662746921\n",
      "Epoch 2: Training Acc = 80.62191358024691 | Val Acc = (0.4890015995502472, 0.7636805555555556) | Training Loss = 0.4283381611955019\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenized text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenize(df_val[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m---> 16\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     val_acc \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Training Acc = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Val Acc = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Training Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[93], line 40\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(model, dataloader)\u001b[0m\n\u001b[0;32m     37\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 40\u001b[0m     preds \u001b[38;5;241m=\u001b[39m (\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     41\u001b[0m     total_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (preds \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     42\u001b[0m     total_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print (torch.__version__)\n",
    "print (torch.cuda.is_available())\n",
    "print (torch.version.cuda)\n",
    "\n",
    "print (df_train[\"sentiment\"].value_counts())\n",
    "print (f\"original text: {df_train['text'].iloc[0]}\")\n",
    "print (f\"Tokenized text: {tokenize(df_train['text'].iloc[0])}\")\n",
    "print (f\"Vocab size: {len(vocab)}\")\n",
    "print (f\"Sample words: {vocab.get_itos()[:20]}\")\n",
    "print(\"------------\")\n",
    "print (df_val[\"sentiment\"].value_counts())\n",
    "print (f\"original text: {df_val['text'].iloc[0]}\")\n",
    "print (f\"Tokenized text: {tokenize(df_val['text'].iloc[0])}\")\n",
    "\n",
    "for epoch in range(5):\n",
    "    train_loss, train_acc = train_loop(model, train_loader)\n",
    "    val_acc = evaluate(model, val_loader)\n",
    "    print (f\"Epoch {epoch+1}: Training Acc = {train_acc} | Val Acc = {val_acc} | Training Loss = {train_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac307585",
   "metadata": {},
   "source": [
    "### 4. Comparisons and Conclusions ðŸ’­"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
